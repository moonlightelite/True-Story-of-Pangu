https://github.com/HW-whistleblower/True-Story-of-Pangu/blob/main/README.md

# Pangu's Sorrow: The Sorrow and Darkness of Huawei's Noah Pangu Large Model R&D Process 

Hello everyone, 

I am an employee of the Pangu Large Model Team and Huawei Noah's Ark Laboratory. 

First of all, to prove my identity, I will list some details: 

1. Wang Yunhe, the current director of Noah, the former director of the Algorithm Application Department, and later renamed the director of the Small Model Laboratory. Former Noah Director: Yao Jun (everyone calls him Teacher Yao). Several laboratory directors: Tang Ruiming (Brother Ming, Team Ming, has resigned), Shang Lifeng, Zhang Wei (Brother Wei), Hao Jianye (Teacher Hao), Liu Wulong (called Wulong Institute), etc. Many other key members and experts have left one after another. 
2. We belong to the "Four Fields" organization. There are many columns under the Fourth Field, and the basic language large model is the fourth column. Wang Yunhe's small model is the sixteenth column. We have participated in the Suzhou assembly, with time nodes in various months. The task order was issued at the Suzhou Research Conference, and the goal needs to be achieved before the node. The Suzhou assembly will gather personnel from all over the country in the Suzhou Research Institute. They usually stay in hotels, such as in Luzhi, and are separated from their families and children. 
3. When assembling in Suzhou, it is defaulted to work on Saturdays, which is very hard, but there is afternoon tea on Saturdays, and there was crayfish once. The workstation in the Suzhou Research Institute has been moved once, from one building to another. The buildings of the Suzhou Research Institute are all decorated in European style, with a big slope at the door, and the scenery inside is very good. It usually takes at least a week to go to Suzhou for assembly, or even longer, and many people can't even go home for a month or two. 
4. Noah was once said to be a research-oriented team, but after coming here, because of the large model project in the Fourth Field Army, the project members have completely become delivery-oriented, and are full of regular meetings, reviews, and reports. Many times, applications are required to do experiments. The team needs to connect to many business lines such as Terminal Xiaoyi, Huawei Cloud, ICT, etc., and the delivery pressure is not small. 
5. The Pangu model developed by Noah was internally code-named "Pangu Zhizi" in the early days. At first, there was only a web version that needed to be applied for internal trials. Later, under pressure, it was connected to Welink and opened for public testing. 

These days, there has been a lot of controversy about the questioning of Pangu's large model plagiarizing Qianwen. As a member of the Pangu team, I have been tossing and turning every night recently and have difficulty falling asleep. Pangu's brand has been affected so much. On the one hand, I selfishly worry about my career development and feel that my past hard work is not worth it. On the other hand, I feel very happy because someone has begun to expose these things. For many days and nights, we have been gnashing our teeth at the behavior of some people inside who have repeatedly relied on fraud and gained countless benefits, but we are powerless. This kind of repression and humiliation has gradually worn away my feelings for Huawei, making my time here gradually muddleheaded, confused and at a loss, and often doubting my life and self-worth. 

I admit that I am a coward. As a small worker, I not only dare not go against Wang Yunhe and other people who have all the power inside, but also dare not go against a behemoth like Huawei. I am afraid of losing my job. After all, I also have a family and children, so I admire the whistleblower from the bottom of my heart. However, when I saw that the internal staff was still trying to cover up the facts and deceive the public, I really couldn't tolerate it. I also hope to be brave once and follow my heart. Even if I lose 800, I hope to hurt the enemy 1,000. I decided to publish what I saw and heard here (part of which came from my colleagues' oral narration), about the "legendary story" of Pangu's large model: 

Huawei does mainly train large models on Ascend cards (there are many NVIDIA cards in the small model laboratory, which they also used for training before, and then transferred to Ascend). I was once impressed by Huawei's determination to "build the world's second choice", and I myself once had a deep affection for Huawei. We accompanied Ascend step by step, from full of bugs to being able to train models now, we paid a lot of effort and cost. 

At the beginning, our computing power was very limited, and we trained models on 910A. At that time, it only supported fp16, and the training stability was far less than bf16. Pangu's moe started very early, and in 23 years it mainly trained 38Bmoe models and subsequent 71B dense models. The dense model of 71B was expanded to become the first generation of 135B dense model, and the main model was gradually trained on 910B. 

Both 71B and 135B models have a huge flaw, which is the tokenizer. The encoding efficiency of the tokenizer used at that time was extremely low, and each single symbol, number, space, and even Chinese character would occupy a token. As you can imagine, this would waste a lot of computing power and make the model's effect very poor. At this time, the small model laboratory happened to have a vocabulary that it had trained itself. Professor Yao suspected that the tokenizer of the model was not good (although in retrospect, his suspicion was undoubtedly correct), so he decided to let 71B and 135B change the tokenizer because the small model laboratory had tried it before. The team stitched two tokenizers and started to replace the tokenizer. The replacement of the 71B model failed, but the 135B model finally succeeded in replacing the vocabulary after training at least 1T of data because it adopted a more sophisticated embedding initialization strategy, but as you can imagine, the effect would not be better. 

At the same time, other domestic companies such as Alibaba and Zhipu were training on GPUs and had already figured out the right method. The gap between Pangu and its competitors was getting bigger and bigger. A dense model with 230B trained from scratch failed to train for various reasons, which almost put the project in a desperate situation. Faced with the pressure of several nodes and strong internal doubts about Pangu, the morale of the team was extremely low. The team made a lot of efforts and struggles when the computing power was extremely limited. For example, the team accidentally discovered that the 38B moe at that time did not have the expected moe effect. So the moe parameter was removed and restored to a 13B dense model. Since the 38B moe originated from the very early pangu alpha 13B and the architecture was relatively backward, the team performed a series of operations, such as switching the absolute position encoding to rope, removing bias, and switching to rmsnorm. At the same time, in view of some failures of the tokenizer and the experience of changing the vocabulary, the vocabulary of this model was also replaced with the vocabulary used by Wang Yunhe's small model laboratory 7B model. The 13B model was then expanded and trained, becoming the second-generation 38B dense model (this model was the main Pangu mid-range model for several months), which was once competitive. However, due to the backward architecture of the larger 135B model and the huge damage caused by the replacement of the vocabulary model (subsequent analysis found that the stitched vocabulary replaced at that time had more serious bugs), after the training, there was still a big gap with the then leading domestic models such as Qianwen. At this time, due to the increasing internal doubts and pressure from the leadership, the team was almost in a desperate situation. 

In this situation, Wang Yunhe and his small model laboratory took action. They claimed that they inherited and transformed from the old 135B parameters, and through training a short few hundred B of data, the average improvement of various indicators was about ten points. In fact, this was their first masterpiece of applying the shell to the large model. Huawei's laymen led the experts, which made the leaders completely unaware of this nonsense. They only thought that there must be some algorithm innovation. After internal analysis, they actually used Qwen 1.5 110B for continued training. By adding layers, expanding the ffn dimension, and adding some mechanisms from the Pangu pi paper, they gathered about 135B parameters. In fact, the old 135B has 107 layers, while this model has only 82 layers, and the various configurations are also different. After training, the distribution of many parameters of the new 135B of unknown origin is almost exactly the same as that of Qwen 110B. Even the class name of the model code was Qwen at the time, and they were too lazy to even change the name. The subsequent model is the so-called 135B V2. This model was also provided to many downstreams at the time, even including external customers.

This incident brought a huge impact to our colleagues who work seriously and honestly. Many people inside actually knew about it, even the terminal and Huawei Cloud. We all joked that we should not call it Pangu model anymore, but Qiangu. At that time, the team members wanted to report it to BCG. After all, this was already a major business fraud. But later it was said that they were stopped by the leaders, because higher-level leaders (such as Mr. Yao, and possibly Mr. Xiong and Mr. Cha) actually knew about it later, but they didn't care, because it was also beneficial for them to get good results through shelling. This incident made several of the strongest colleagues in the team at that time begin to become discouraged, and leaving the job and running away gradually became a matter of conversation. 

At this time, Pangu seemed to have ushered in a turnaround. Since the Pangu models mentioned above were basically continued training and transformation, Noah did not master the technology of training from scratch at that time, not to mention training on Ascend's NPU. With the efforts of the core members of the team at that time, Pangu started training the third-generation model. After great efforts, it gradually aligned with the industry in terms of data architecture and training algorithms. The hardship had nothing to do with the people in the small model laboratory. 

At the beginning, the team members had no confidence and started training with only a 13B model, but later found that the effect was not bad, so the model was expanded again and became the third-generation 38B, codenamed 38B V3. I believe that many brothers in the product line are familiar with this model. At that time, the tokenizer of this model was expanded based on the llama vocabulary (which is also a common practice in the industry). At that time, Wang Yunhe's laboratory made another vocabulary (that is, the vocabulary of the subsequent pangu series). At that time, the two vocabulary were forced to compete, and there was no obvious conclusion of good or bad. Therefore, the leader immediately decided that the vocabulary should be unified and used by Wang Yunhe and his team. Therefore, the 135B V3 (that is, Pangu Ultra, which was trained from scratch later) adopted this tokenizer. This also explains the confusion of many brothers who use our model, why two different models of the same V3 generation use different tokenizers. 


We feel from the bottom of our hearts that 135B V3 is the pride of our 4th vertical team at that time. This is the first truly self-developed model of Huawei's full stack, which is trained from scratch with a capacity of hundreds of billions, and its effect is comparable to that of competitors in the same period of 24 years. I am already in tears when writing this, it is not easy. At that time, in order to stabilize the training, the team did a lot of experimental comparisons, and repeatedly rolled back and restarted in time when the model gradient was abnormal. This model really achieved the technical report mentioned later that there was no loss spike throughout the training process. We overcame countless difficulties, and we did it. We are willing to use our lives and honor to ensure the authenticity of this model training. How many early mornings did we stay awake for its training. When we were scolded as worthless by internal voices, how unwilling we were, how many grievances we had, we stood up. 

We are really burning our youth to polish the domestic computing power base... Living in a foreign land, we gave up our families, vacations, health, entertainment, and sacrificed our lives. The hardships and difficulties are not enough to be summarized in a few words. At various mobilization meetings, we were deeply moved by the slogans of Pangu and Huawei. 

However, all our hard work was often taken away by the small model laboratory. The data was taken directly. The code was taken directly, and we were required to match it to run with one click. We jokingly called the small model laboratory the mouse-clicking laboratory. We worked hard and they achieved glory. It really fits the saying that you are carrying a heavy burden because someone is keeping you safe. In this situation, more and more comrades-in-arms can no longer hold on and choose to leave. Seeing those excellent colleagues around me leave one by one, I feel sad and sad. In this combat-like environment, we are more like comrades-in-arms than colleagues. They also have countless things to learn from them in technology, and they are good teachers. Seeing them go to many outstanding teams such as Byte Seed, Deepseek, Dark Side of the Moon, Tencent and Kuaishou, I am happy and wish them well from the bottom of my heart, leaving this hard but dirty place. I still remember the words of a colleague who left, he said: "Coming here is a shame in my technical career, and staying here every day is a waste of life." Although the words are unpleasant, I am speechless. I am worried that my technical accumulation is insufficient and I can't adapt to the high elimination environment of Internet companies. I have wanted to leave many times but never took this step. 

In addition to the dense model, Pangu also started the exploration of moe. At the beginning, a 224B moe model was trained. In parallel, the small model laboratory also started the second major shell action (secondary episodes may also include some other models, such as math models), that is, the pangu pro moe 72B that is widely circulated this time. This model claims to be expanded from the 7B of the small model laboratory (even so, this is inconsistent with the technical report, not to mention the 14B training of the shell qwen 2.5). I still remember that they had trained for a few days, and the internal evaluation immediately caught up with the 38B V3 at that time. Many brothers in the AI ​​System Laboratory knew about their shelling because they needed to adapt the model, but they were unable to do justice due to various reasons. In fact, I was surprised that Honestagi could analyze this level of similarity for this model that had been trained for a long time, because the computing power spent on this model to continue training and washing parameters was even enough to train a model of the same level from scratch. I heard from my colleagues that they took a lot of measures to wash off the watermark of Qianwen, even including deliberately training dirty data. This also provides an unprecedented special model for the academic community to study model lineage. In the future, new lineage methods can be proposed and used. 

At the end of 24 and the beginning of 25, after the release of Deepseek v3 and r1, the team was greatly shocked and questioned due to its amazing technical level. So in order to keep up with the trend, Pangu imitated the model size of Deepseek and started training 718B moe. At this time, the small model laboratory took action again. They chose to continue training with Deepseekv3 shell. They performed training by freezing the parameters loaded by Deepseek. Even the directory for loading ckpt in the task is deepseekv3, and they did not change it. How arrogant? On the contrary, some colleagues with real technical beliefs are training another 718B moe from scratch. But there are all kinds of problems. But it is obvious that how can this model be better than directly shelling? If it were not for the insistence of the team leader, it would have been stopped long ago. 

Huawei's heavy process management has seriously slowed down the development rhythm of large models, such as version management, model lineage, various processes, and various traceability. Ironically, the models of the small model laboratory seem to have never been constrained by these processes. They can shell if they want to, and continue training if they want to, and the computing power is constantly reaching out. This strong and almost magical contrast illustrates the current situation of process management: only the state officials are allowed to set fires, but the people are not allowed to light lamps. How ridiculous? How sad? How hateful? How shameful! 

After the HonestAGI incident came out, we asked everyone to keep discussing and analyzing how to do public relations and "respond". Admittedly, the analysis of this original article may not be strong enough, giving Wang Yunhe and the small model laboratory the opportunity to quibble and confuse right and wrong. For this reason, I felt sick in my heart these two days, and I always doubted the meaning of my life and the blindness of heaven. I won't accompany you anymore. I will resign. At the same time, I am also applying to be removed from the list of authors of some Pangu technical reports. Having signed on these technical reports is a stain that I will never be able to erase in my life. At that time, I didn't expect that they would be so rampant as to dare to open source. I didn't expect that they would dare to fool the world so much and publicize it. At that time, I may have been lucky and did not refuse to sign. I believe that many comrades who work hard were just forced to board the pirate ship, or they were unaware of it. But this matter can no longer be undone. I hope that I can persist in doing truly meaningful things for the rest of my life and atone for my weakness and indecision at that time.

Writing this late at night, I am already in tears and sobbing. I still remember when some outstanding colleagues left, I asked them with a wry smile whether they wanted to post a long and customary post to reveal the current situation. The other party said: No, it's a waste of time, and I'm also afraid that you will be worse off if I reveal it. I was suddenly sad at the time, because the comrades who once fought for their ideals together had completely lost hope in Huawei. At that time, everyone joked that we were using the millet and rifles of the Communist Party, but the organization had a style comparable to that of the Kuomintang. 

Once upon a time, I was proud that we used millet and rifles to defeat foreign guns and cannons. 

Now, I'm tired and I want to surrender. 

In fact, to this day, I still sincerely hope that Huawei can seriously learn from its lessons, do a good job of Pangu, make Pangu world-class, and turn Ascend into NVIDIA's level. The internal bad money drives out the good money, causing Noah and even Huawei to lose a large number of outstanding large model talents in a short period of time. I believe that they are also shining in various teams such as Deepseek, displaying their ambitions and talents, and contributing to the fierce competition between China and the United States in AI. I often lament that Huawei is not without talent, but simply does not know how to retain talent. If these people are given the right environment, the right resources, fewer shackles, and less political struggles, Pangu will be successful. 

Finally: I swear on my life, personality, and honor that all the above content I wrote is true (at least within my limited knowledge). I do not have such a high level of technology and the opportunity to do a detailed and solid analysis, and I dare not directly use internal records to provide evidence, fearing that I will be caught because of information security. But I believe that many of my former comrades will testify for me. Brothers within Huawei, including brothers in the product lines we have served, I believe that the countless details in this article can be compared with your impressions and confirm my statement. You may have been deceived, but these cruel truths will not be covered in dust. The traces of our struggle should not be distorted and buried. 

After writing so much, some people must want to find me out and erase me. The company may also want to silence me or even hold me accountable. If this is true, the personal and even life safety of me and my family may be threatened. In order to protect myself, I will report my safety to everyone every day in the near future. 

If I disappear, I will consider myself as a sacrifice for the truth and ideals, for Huawei and even China to better develop computing power and AI. I would like to be buried in the place where I once struggled. 

Goodbye, Noah. 

Written in Shenzhen in the early morning of July 6, 2025 

--- 

Hello everyone, 

thank you for your concern and blessings. I am temporarily safe, but the company should be conducting investigations and collecting certain lists. The follow-up situation is unknown. 

I will add some details to prevent some people from continuing to confuse right and wrong. 

Regarding 135B V2, after the small model laboratory quickly completed the shell and took all the benefits brought by the shell (such as task order commendation and timely incentives), because it did not want to continue to support downstream applications and model iterations, it threw this hot potato to the Fourth Column. It is indeed a step ahead in skills, directly dragging the brothers of the Fourth Column into the water. A colleague provided an old model in the past, and finally got back a magic modified advanced Qianwen at that time. People who make large models are as familiar with the models they make as their own children. Don't treat others as fools. Just like your own son went out for a trip, and came back as someone else's child. 

The signature of Pangu report does not conform to academic norms. For example, there are many people who have made technical contributions to 135B V3. Due to the limited number of authors, the fruits of their labor have not been properly rewarded, and there have been many complaints within the team. This model was the crystallization of everyone's wisdom and sweat at the time, and even the spiritual pillar of the team at the time, supporting many brothers to continue to stay in Noah. The so-called quota limit, as well as the inclusion of some people who have made no technical contributions (such as some people in the small model laboratory), made the brothers very disappointed.

